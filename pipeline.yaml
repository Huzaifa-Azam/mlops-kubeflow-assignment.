apiVersion: argoproj.io/v1alpha1
kind: Workflow
metadata:
  generateName: boston-housing-pipeline-
  annotations: {pipelines.kubeflow.org/kfp_sdk_version: 1.8.23, pipelines.kubeflow.org/pipeline_compilation_time: '2025-11-29T15:59:51.341685',
    pipelines.kubeflow.org/pipeline_spec: '{"description": "A pipeline to train and
      evaluate a model on the Boston housing dataset.", "inputs": [{"default": "https://raw.githubusercontent.com/ageron/handson-ml/master/datasets/housing/housing.tgz",
      "name": "data_url", "optional": true, "type": "String"}], "name": "Boston Housing
      Pipeline"}'}
  labels: {pipelines.kubeflow.org/kfp_sdk_version: 1.8.23}
spec:
  entrypoint: boston-housing-pipeline
  templates:
  - name: boston-housing-pipeline
    inputs:
      parameters:
      - {name: data_url}
    dag:
      tasks:
      - name: data-extraction
        template: data-extraction
        arguments:
          parameters:
          - {name: data_url, value: '{{inputs.parameters.data_url}}'}
      - name: data-preprocessing
        template: data-preprocessing
        dependencies: [data-extraction]
        arguments:
          artifacts:
          - {name: data-extraction-output_csv, from: '{{tasks.data-extraction.outputs.artifacts.data-extraction-output_csv}}'}
      - name: model-evaluation
        template: model-evaluation
        dependencies: [data-preprocessing, model-training]
        arguments:
          artifacts:
          - {name: data-preprocessing-test_csv, from: '{{tasks.data-preprocessing.outputs.artifacts.data-preprocessing-test_csv}}'}
          - {name: model-training-model_pkl, from: '{{tasks.model-training.outputs.artifacts.model-training-model_pkl}}'}
      - name: model-training
        template: model-training
        dependencies: [data-preprocessing]
        arguments:
          artifacts:
          - {name: data-preprocessing-train_csv, from: '{{tasks.data-preprocessing.outputs.artifacts.data-preprocessing-train_csv}}'}
  - name: data-extraction
    container:
      args: [--data-url, '{{inputs.parameters.data_url}}', --output-csv, /tmp/outputs/output_csv/data]
      command:
      - sh
      - -c
      - (PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location
        'pandas' 'scikit-learn' || PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip
        install --quiet --no-warn-script-location 'pandas' 'scikit-learn' --user)
        && "$0" "$@"
      - sh
      - -ec
      - |
        program_path=$(mktemp)
        printf "%s" "$0" > "$program_path"
        python3 -u "$program_path" "$@"
      - "def _make_parent_dirs_and_return_path(file_path: str):\n    import os\n \
        \   os.makedirs(os.path.dirname(file_path), exist_ok=True)\n    return file_path\n\
        \ndef data_extraction(data_url, output_csv):\n    \"\"\"\n    Downloads the\
        \ dataset from a source (simulating DVC get/import or direct download).\n\
        \    For this assignment, we'll fetch the California housing dataset directly\
        \ \n    to ensure it works in the isolated container environment.\n    \"\"\
        \"\n    import pandas as pd\n    from sklearn.datasets import fetch_california_housing\n\
        \n    print(f\"Fetching data from {data_url}...\")\n    # In a real DVC scenario,\
        \ we would use:\n    # import dvc.api\n    # with dvc.api.open(data_url) as\
        \ f:\n    #     df = pd.read_csv(f)\n\n    # For simplicity and robustness\
        \ in this demo:\n    data = fetch_california_housing()\n    df = pd.DataFrame(data.data,\
        \ columns=data.feature_names)\n    df['target'] = data.target\n\n    df.to_csv(output_csv,\
        \ index=False)\n    print(f\"Data saved to {output_csv}\")\n\nimport argparse\n\
        _parser = argparse.ArgumentParser(prog='Data extraction', description='Downloads\
        \ the dataset from a source (simulating DVC get/import or direct download).')\n\
        _parser.add_argument(\"--data-url\", dest=\"data_url\", type=str, required=True,\
        \ default=argparse.SUPPRESS)\n_parser.add_argument(\"--output-csv\", dest=\"\
        output_csv\", type=_make_parent_dirs_and_return_path, required=True, default=argparse.SUPPRESS)\n\
        _parsed_args = vars(_parser.parse_args())\n\n_outputs = data_extraction(**_parsed_args)\n"
      image: python:3.8
    inputs:
      parameters:
      - {name: data_url}
    outputs:
      artifacts:
      - {name: data-extraction-output_csv, path: /tmp/outputs/output_csv/data}
    metadata:
      labels:
        pipelines.kubeflow.org/kfp_sdk_version: 1.8.23
        pipelines.kubeflow.org/pipeline-sdk-type: kfp
        pipelines.kubeflow.org/enable_caching: "true"
      annotations: {pipelines.kubeflow.org/component_spec: '{"description": "Downloads
          the dataset from a source (simulating DVC get/import or direct download).",
          "implementation": {"container": {"args": ["--data-url", {"inputValue": "data_url"},
          "--output-csv", {"outputPath": "output_csv"}], "command": ["sh", "-c", "(PIP_DISABLE_PIP_VERSION_CHECK=1
          python3 -m pip install --quiet --no-warn-script-location ''pandas'' ''scikit-learn''
          || PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location
          ''pandas'' ''scikit-learn'' --user) && \"$0\" \"$@\"", "sh", "-ec", "program_path=$(mktemp)\nprintf
          \"%s\" \"$0\" > \"$program_path\"\npython3 -u \"$program_path\" \"$@\"\n",
          "def _make_parent_dirs_and_return_path(file_path: str):\n    import os\n    os.makedirs(os.path.dirname(file_path),
          exist_ok=True)\n    return file_path\n\ndef data_extraction(data_url, output_csv):\n    \"\"\"\n    Downloads
          the dataset from a source (simulating DVC get/import or direct download).\n    For
          this assignment, we''ll fetch the California housing dataset directly \n    to
          ensure it works in the isolated container environment.\n    \"\"\"\n    import
          pandas as pd\n    from sklearn.datasets import fetch_california_housing\n\n    print(f\"Fetching
          data from {data_url}...\")\n    # In a real DVC scenario, we would use:\n    #
          import dvc.api\n    # with dvc.api.open(data_url) as f:\n    #     df =
          pd.read_csv(f)\n\n    # For simplicity and robustness in this demo:\n    data
          = fetch_california_housing()\n    df = pd.DataFrame(data.data, columns=data.feature_names)\n    df[''target'']
          = data.target\n\n    df.to_csv(output_csv, index=False)\n    print(f\"Data
          saved to {output_csv}\")\n\nimport argparse\n_parser = argparse.ArgumentParser(prog=''Data
          extraction'', description=''Downloads the dataset from a source (simulating
          DVC get/import or direct download).'')\n_parser.add_argument(\"--data-url\",
          dest=\"data_url\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--output-csv\",
          dest=\"output_csv\", type=_make_parent_dirs_and_return_path, required=True,
          default=argparse.SUPPRESS)\n_parsed_args = vars(_parser.parse_args())\n\n_outputs
          = data_extraction(**_parsed_args)\n"], "image": "python:3.8"}}, "inputs":
          [{"name": "data_url", "type": "String"}], "name": "Data extraction", "outputs":
          [{"name": "output_csv", "type": "String"}]}', pipelines.kubeflow.org/component_ref: '{"digest":
          "ed1d9d2a7d710af463b7f2c48e8c7d88f7fb8bd8046874768aeed72f51eb36e6", "url":
          "components/data_extraction.yaml"}', pipelines.kubeflow.org/arguments.parameters: '{"data_url":
          "{{inputs.parameters.data_url}}"}'}
  - name: data-preprocessing
    container:
      args: [--input-csv, /tmp/inputs/input_csv/data, --train-csv, /tmp/outputs/train_csv/data,
        --test-csv, /tmp/outputs/test_csv/data]
      command:
      - sh
      - -c
      - (PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location
        'pandas' 'scikit-learn' || PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip
        install --quiet --no-warn-script-location 'pandas' 'scikit-learn' --user)
        && "$0" "$@"
      - sh
      - -ec
      - |
        program_path=$(mktemp)
        printf "%s" "$0" > "$program_path"
        python3 -u "$program_path" "$@"
      - "def _make_parent_dirs_and_return_path(file_path: str):\n    import os\n \
        \   os.makedirs(os.path.dirname(file_path), exist_ok=True)\n    return file_path\n\
        \ndef data_preprocessing(input_csv, \n                       train_csv, \n\
        \                       test_csv):\n    \"\"\"\n    Loads data, cleans it,\
        \ and splits into train/test sets.\n    \"\"\"\n    import pandas as pd\n\
        \    from sklearn.model_selection import train_test_split\n\n    print(\"\
        Loading data...\")\n    df = pd.read_csv(input_csv)\n\n    # Simple preprocessing:\
        \ Drop rows with missing values (if any)\n    df.dropna(inplace=True)\n\n\
        \    print(\"Splitting data...\")\n    train_df, test_df = train_test_split(df,\
        \ test_size=0.2, random_state=42)\n\n    train_df.to_csv(train_csv, index=False)\n\
        \    test_df.to_csv(test_csv, index=False)\n    print(\"Data split and saved.\"\
        )\n\nimport argparse\n_parser = argparse.ArgumentParser(prog='Data preprocessing',\
        \ description='Loads data, cleans it, and splits into train/test sets.')\n\
        _parser.add_argument(\"--input-csv\", dest=\"input_csv\", type=str, required=True,\
        \ default=argparse.SUPPRESS)\n_parser.add_argument(\"--train-csv\", dest=\"\
        train_csv\", type=_make_parent_dirs_and_return_path, required=True, default=argparse.SUPPRESS)\n\
        _parser.add_argument(\"--test-csv\", dest=\"test_csv\", type=_make_parent_dirs_and_return_path,\
        \ required=True, default=argparse.SUPPRESS)\n_parsed_args = vars(_parser.parse_args())\n\
        \n_outputs = data_preprocessing(**_parsed_args)\n"
      image: python:3.8
    inputs:
      artifacts:
      - {name: data-extraction-output_csv, path: /tmp/inputs/input_csv/data}
    outputs:
      artifacts:
      - {name: data-preprocessing-test_csv, path: /tmp/outputs/test_csv/data}
      - {name: data-preprocessing-train_csv, path: /tmp/outputs/train_csv/data}
    metadata:
      labels:
        pipelines.kubeflow.org/kfp_sdk_version: 1.8.23
        pipelines.kubeflow.org/pipeline-sdk-type: kfp
        pipelines.kubeflow.org/enable_caching: "true"
      annotations: {pipelines.kubeflow.org/component_spec: '{"description": "Loads
          data, cleans it, and splits into train/test sets.", "implementation": {"container":
          {"args": ["--input-csv", {"inputPath": "input_csv"}, "--train-csv", {"outputPath":
          "train_csv"}, "--test-csv", {"outputPath": "test_csv"}], "command": ["sh",
          "-c", "(PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location
          ''pandas'' ''scikit-learn'' || PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m
          pip install --quiet --no-warn-script-location ''pandas'' ''scikit-learn''
          --user) && \"$0\" \"$@\"", "sh", "-ec", "program_path=$(mktemp)\nprintf
          \"%s\" \"$0\" > \"$program_path\"\npython3 -u \"$program_path\" \"$@\"\n",
          "def _make_parent_dirs_and_return_path(file_path: str):\n    import os\n    os.makedirs(os.path.dirname(file_path),
          exist_ok=True)\n    return file_path\n\ndef data_preprocessing(input_csv,
          \n                       train_csv, \n                       test_csv):\n    \"\"\"\n    Loads
          data, cleans it, and splits into train/test sets.\n    \"\"\"\n    import
          pandas as pd\n    from sklearn.model_selection import train_test_split\n\n    print(\"Loading
          data...\")\n    df = pd.read_csv(input_csv)\n\n    # Simple preprocessing:
          Drop rows with missing values (if any)\n    df.dropna(inplace=True)\n\n    print(\"Splitting
          data...\")\n    train_df, test_df = train_test_split(df, test_size=0.2,
          random_state=42)\n\n    train_df.to_csv(train_csv, index=False)\n    test_df.to_csv(test_csv,
          index=False)\n    print(\"Data split and saved.\")\n\nimport argparse\n_parser
          = argparse.ArgumentParser(prog=''Data preprocessing'', description=''Loads
          data, cleans it, and splits into train/test sets.'')\n_parser.add_argument(\"--input-csv\",
          dest=\"input_csv\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--train-csv\",
          dest=\"train_csv\", type=_make_parent_dirs_and_return_path, required=True,
          default=argparse.SUPPRESS)\n_parser.add_argument(\"--test-csv\", dest=\"test_csv\",
          type=_make_parent_dirs_and_return_path, required=True, default=argparse.SUPPRESS)\n_parsed_args
          = vars(_parser.parse_args())\n\n_outputs = data_preprocessing(**_parsed_args)\n"],
          "image": "python:3.8"}}, "inputs": [{"name": "input_csv", "type": "String"}],
          "name": "Data preprocessing", "outputs": [{"name": "train_csv", "type":
          "String"}, {"name": "test_csv", "type": "String"}]}', pipelines.kubeflow.org/component_ref: '{"digest":
          "dc7c432ba9b05891942d3d2bc35551453431cb67828941f0ea310dc68d19e361", "url":
          "components/data_preprocessing.yaml"}'}
  - name: model-evaluation
    container:
      args: [--test-csv, /tmp/inputs/test_csv/data, --model-pkl, /tmp/inputs/model_pkl/data,
        --metrics-json, /tmp/outputs/metrics_json/data]
      command:
      - sh
      - -c
      - (PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location
        'pandas' 'scikit-learn' 'joblib' || PIP_DISABLE_PIP_VERSION_CHECK=1 python3
        -m pip install --quiet --no-warn-script-location 'pandas' 'scikit-learn' 'joblib'
        --user) && "$0" "$@"
      - sh
      - -ec
      - |
        program_path=$(mktemp)
        printf "%s" "$0" > "$program_path"
        python3 -u "$program_path" "$@"
      - "def _make_parent_dirs_and_return_path(file_path: str):\n    import os\n \
        \   os.makedirs(os.path.dirname(file_path), exist_ok=True)\n    return file_path\n\
        \ndef model_evaluation(test_csv, \n                     model_pkl,\n     \
        \                metrics_json):\n    \"\"\"\n    Evaluates the model and saves\
        \ metrics.\n    \"\"\"\n    import pandas as pd\n    import joblib\n    import\
        \ json\n    from sklearn.metrics import mean_squared_error, r2_score\n\n \
        \   print(\"Loading test data and model...\")\n    df = pd.read_csv(test_csv)\n\
        \    X_test = df.drop('target', axis=1)\n    y_test = df['target']\n\n   \
        \ model = joblib.load(model_pkl)\n\n    print(\"Predicting...\")\n    y_pred\
        \ = model.predict(X_test)\n\n    mse = mean_squared_error(y_test, y_pred)\n\
        \    r2 = r2_score(y_test, y_pred)\n\n    metrics = {\n        'mse': mse,\n\
        \        'r2': r2\n    }\n\n    print(f\"Metrics: {metrics}\")\n\n    with\
        \ open(metrics_json, 'w') as f:\n        json.dump(metrics, f)\n    print(f\"\
        Metrics saved to {metrics_json}\")\n\nimport argparse\n_parser = argparse.ArgumentParser(prog='Model\
        \ evaluation', description='Evaluates the model and saves metrics.')\n_parser.add_argument(\"\
        --test-csv\", dest=\"test_csv\", type=str, required=True, default=argparse.SUPPRESS)\n\
        _parser.add_argument(\"--model-pkl\", dest=\"model_pkl\", type=str, required=True,\
        \ default=argparse.SUPPRESS)\n_parser.add_argument(\"--metrics-json\", dest=\"\
        metrics_json\", type=_make_parent_dirs_and_return_path, required=True, default=argparse.SUPPRESS)\n\
        _parsed_args = vars(_parser.parse_args())\n\n_outputs = model_evaluation(**_parsed_args)\n"
      image: python:3.8
    inputs:
      artifacts:
      - {name: model-training-model_pkl, path: /tmp/inputs/model_pkl/data}
      - {name: data-preprocessing-test_csv, path: /tmp/inputs/test_csv/data}
    outputs:
      artifacts:
      - {name: model-evaluation-metrics_json, path: /tmp/outputs/metrics_json/data}
    metadata:
      labels:
        pipelines.kubeflow.org/kfp_sdk_version: 1.8.23
        pipelines.kubeflow.org/pipeline-sdk-type: kfp
        pipelines.kubeflow.org/enable_caching: "true"
      annotations: {pipelines.kubeflow.org/component_spec: '{"description": "Evaluates
          the model and saves metrics.", "implementation": {"container": {"args":
          ["--test-csv", {"inputPath": "test_csv"}, "--model-pkl", {"inputPath": "model_pkl"},
          "--metrics-json", {"outputPath": "metrics_json"}], "command": ["sh", "-c",
          "(PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location
          ''pandas'' ''scikit-learn'' ''joblib'' || PIP_DISABLE_PIP_VERSION_CHECK=1
          python3 -m pip install --quiet --no-warn-script-location ''pandas'' ''scikit-learn''
          ''joblib'' --user) && \"$0\" \"$@\"", "sh", "-ec", "program_path=$(mktemp)\nprintf
          \"%s\" \"$0\" > \"$program_path\"\npython3 -u \"$program_path\" \"$@\"\n",
          "def _make_parent_dirs_and_return_path(file_path: str):\n    import os\n    os.makedirs(os.path.dirname(file_path),
          exist_ok=True)\n    return file_path\n\ndef model_evaluation(test_csv, \n                     model_pkl,\n                     metrics_json):\n    \"\"\"\n    Evaluates
          the model and saves metrics.\n    \"\"\"\n    import pandas as pd\n    import
          joblib\n    import json\n    from sklearn.metrics import mean_squared_error,
          r2_score\n\n    print(\"Loading test data and model...\")\n    df = pd.read_csv(test_csv)\n    X_test
          = df.drop(''target'', axis=1)\n    y_test = df[''target'']\n\n    model
          = joblib.load(model_pkl)\n\n    print(\"Predicting...\")\n    y_pred = model.predict(X_test)\n\n    mse
          = mean_squared_error(y_test, y_pred)\n    r2 = r2_score(y_test, y_pred)\n\n    metrics
          = {\n        ''mse'': mse,\n        ''r2'': r2\n    }\n\n    print(f\"Metrics:
          {metrics}\")\n\n    with open(metrics_json, ''w'') as f:\n        json.dump(metrics,
          f)\n    print(f\"Metrics saved to {metrics_json}\")\n\nimport argparse\n_parser
          = argparse.ArgumentParser(prog=''Model evaluation'', description=''Evaluates
          the model and saves metrics.'')\n_parser.add_argument(\"--test-csv\", dest=\"test_csv\",
          type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--model-pkl\",
          dest=\"model_pkl\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--metrics-json\",
          dest=\"metrics_json\", type=_make_parent_dirs_and_return_path, required=True,
          default=argparse.SUPPRESS)\n_parsed_args = vars(_parser.parse_args())\n\n_outputs
          = model_evaluation(**_parsed_args)\n"], "image": "python:3.8"}}, "inputs":
          [{"name": "test_csv", "type": "String"}, {"name": "model_pkl", "type": "String"}],
          "name": "Model evaluation", "outputs": [{"name": "metrics_json", "type":
          "String"}]}', pipelines.kubeflow.org/component_ref: '{"digest": "7f9832492bb9117a6bf20f862f975dfcd334da97a1bd2fb527a1292111a966e6",
          "url": "components/model_evaluation.yaml"}'}
  - name: model-training
    container:
      args: [--train-csv, /tmp/inputs/train_csv/data, --model-pkl, /tmp/outputs/model_pkl/data]
      command:
      - sh
      - -c
      - (PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location
        'pandas' 'scikit-learn' 'joblib' || PIP_DISABLE_PIP_VERSION_CHECK=1 python3
        -m pip install --quiet --no-warn-script-location 'pandas' 'scikit-learn' 'joblib'
        --user) && "$0" "$@"
      - sh
      - -ec
      - |
        program_path=$(mktemp)
        printf "%s" "$0" > "$program_path"
        python3 -u "$program_path" "$@"
      - "def _make_parent_dirs_and_return_path(file_path: str):\n    import os\n \
        \   os.makedirs(os.path.dirname(file_path), exist_ok=True)\n    return file_path\n\
        \ndef model_training(train_csv, \n                   model_pkl):\n    \"\"\
        \"\n    Trains a Random Forest classifier (regressor for housing data) and\
        \ saves the model.\n    \"\"\"\n    import pandas as pd\n    import joblib\n\
        \    from sklearn.ensemble import RandomForestRegressor\n\n    print(\"Loading\
        \ training data...\")\n    df = pd.read_csv(train_csv)\n    X = df.drop('target',\
        \ axis=1)\n    y = df['target']\n\n    print(\"Training model...\")\n    model\
        \ = RandomForestRegressor(n_estimators=100, random_state=42)\n    model.fit(X,\
        \ y)\n\n    print(\"Saving model...\")\n    joblib.dump(model, model_pkl)\n\
        \    print(f\"Model saved to {model_pkl}\")\n\nimport argparse\n_parser =\
        \ argparse.ArgumentParser(prog='Model training', description='Trains a Random\
        \ Forest classifier (regressor for housing data) and saves the model.')\n\
        _parser.add_argument(\"--train-csv\", dest=\"train_csv\", type=str, required=True,\
        \ default=argparse.SUPPRESS)\n_parser.add_argument(\"--model-pkl\", dest=\"\
        model_pkl\", type=_make_parent_dirs_and_return_path, required=True, default=argparse.SUPPRESS)\n\
        _parsed_args = vars(_parser.parse_args())\n\n_outputs = model_training(**_parsed_args)\n"
      image: python:3.8
    inputs:
      artifacts:
      - {name: data-preprocessing-train_csv, path: /tmp/inputs/train_csv/data}
    outputs:
      artifacts:
      - {name: model-training-model_pkl, path: /tmp/outputs/model_pkl/data}
    metadata:
      labels:
        pipelines.kubeflow.org/kfp_sdk_version: 1.8.23
        pipelines.kubeflow.org/pipeline-sdk-type: kfp
        pipelines.kubeflow.org/enable_caching: "true"
      annotations: {pipelines.kubeflow.org/component_spec: '{"description": "Trains
          a Random Forest classifier (regressor for housing data) and saves the model.",
          "implementation": {"container": {"args": ["--train-csv", {"inputPath": "train_csv"},
          "--model-pkl", {"outputPath": "model_pkl"}], "command": ["sh", "-c", "(PIP_DISABLE_PIP_VERSION_CHECK=1
          python3 -m pip install --quiet --no-warn-script-location ''pandas'' ''scikit-learn''
          ''joblib'' || PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet
          --no-warn-script-location ''pandas'' ''scikit-learn'' ''joblib'' --user)
          && \"$0\" \"$@\"", "sh", "-ec", "program_path=$(mktemp)\nprintf \"%s\" \"$0\"
          > \"$program_path\"\npython3 -u \"$program_path\" \"$@\"\n", "def _make_parent_dirs_and_return_path(file_path:
          str):\n    import os\n    os.makedirs(os.path.dirname(file_path), exist_ok=True)\n    return
          file_path\n\ndef model_training(train_csv, \n                   model_pkl):\n    \"\"\"\n    Trains
          a Random Forest classifier (regressor for housing data) and saves the model.\n    \"\"\"\n    import
          pandas as pd\n    import joblib\n    from sklearn.ensemble import RandomForestRegressor\n\n    print(\"Loading
          training data...\")\n    df = pd.read_csv(train_csv)\n    X = df.drop(''target'',
          axis=1)\n    y = df[''target'']\n\n    print(\"Training model...\")\n    model
          = RandomForestRegressor(n_estimators=100, random_state=42)\n    model.fit(X,
          y)\n\n    print(\"Saving model...\")\n    joblib.dump(model, model_pkl)\n    print(f\"Model
          saved to {model_pkl}\")\n\nimport argparse\n_parser = argparse.ArgumentParser(prog=''Model
          training'', description=''Trains a Random Forest classifier (regressor for
          housing data) and saves the model.'')\n_parser.add_argument(\"--train-csv\",
          dest=\"train_csv\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--model-pkl\",
          dest=\"model_pkl\", type=_make_parent_dirs_and_return_path, required=True,
          default=argparse.SUPPRESS)\n_parsed_args = vars(_parser.parse_args())\n\n_outputs
          = model_training(**_parsed_args)\n"], "image": "python:3.8"}}, "inputs":
          [{"name": "train_csv", "type": "String"}], "name": "Model training", "outputs":
          [{"name": "model_pkl", "type": "String"}]}', pipelines.kubeflow.org/component_ref: '{"digest":
          "e21482673f508b0faa7712fa2275e89488c2e2dd53deac090bfee9515b76011d", "url":
          "components/model_training.yaml"}'}
  arguments:
    parameters:
    - {name: data_url, value: 'https://raw.githubusercontent.com/ageron/handson-ml/master/datasets/housing/housing.tgz'}
  serviceAccountName: pipeline-runner
